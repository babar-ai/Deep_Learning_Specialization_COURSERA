{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning is the process of optimizing the \"settings\" of your neural network that aren’t learned during training. These include factors like learning rate, number of layers, neurons per layer, dropout rate, batch size, optimizer type, and so on. The goal is to find the configuration that gives the best performance on your validation set\n",
        "\n",
        "# Key Steps for Hyperparameter Tuning\n",
        "\n",
        "\n",
        "1.   Define Your Search Space:\n",
        "2.   Choose a Search Strategy:\n",
        "\n",
        "\n",
        "*   Grid Search: Exhaustively try every combination in the defined space (can be computationally expensive).\n",
        "*   \n",
        "Random Search: Randomly sample hyperparameter combinations; often more efficient.\n",
        "\n",
        "\n",
        "* Bayesian Optimization/Hyperband: More advanced methods that can intelligently explore the space based on past evaluations.\n",
        "\n",
        "3. Automate the Process:\n",
        "Use libraries or frameworks that facilitate hyperparameter tuning, such as:\n",
        "\n",
        "* Keras Tuner for TensorFlow/Keras models.\n",
        "* Hyperopt or Optuna for broader use.\n",
        "* Ray Tune for distributed hyperparameter tuning.\n",
        "\n",
        "4. Evaluate and Iterate:\n",
        "* Run your tuning process, review the best-performing configurations, and then consider further fine-tuning within the promising hyperparameter ranges."
      ],
      "metadata": {
        "id": "L7svuOgWEW8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlrSYumIEYnx",
        "outputId": "1b166292-6af7-44d0-9817-2da106049505"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m92.2/129.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g8LHxyR7DXJ",
        "outputId": "25ae89d3-fc04-471d-e6ec-c7a8cd9fefa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 35s]\n",
            "val_accuracy: 0.9333000183105469\n",
            "\n",
            "Best val_accuracy So Far: 0.9778000116348267\n",
            "Total elapsed time: 00h 06m 23s\n",
            "Best learning rate: 0.001\n",
            "Best number of layers: 2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import kerastuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Tune the number of layers.\n",
        "    for i in range(hp.Int('num_layers', 2, 4)):\n",
        "        # Tune the number of units in each layer.\n",
        "        model.add(layers.Dense(units=hp.Int(f'units_{i}', min_value=64, max_value=256, step=64),\n",
        "                               activation='relu'))\n",
        "        # Optionally add dropout.\n",
        "        model.add(layers.Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)))\n",
        "\n",
        "    model.add(layers.Dense(10, activation='softmax'))  # For example, 10 classes.\n",
        "\n",
        "    # Tune the learning rate for the optimizer.\n",
        "    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Instantiate the tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='hyperparam_tuning_example'\n",
        ")\n",
        "\n",
        "# Prepare your data\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28 * 28) / 255.0\n",
        "x_val = x_val.reshape(-1, 28 * 28) / 255.0\n",
        "\n",
        "# Run the hyperparameter search\n",
        "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val))\n",
        "\n",
        "# Get the best model hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best learning rate: {best_hps.get('learning_rate')}\")\n",
        "print(f\"Best number of layers: {best_hps.get('num_layers')}\")\n"
      ]
    }
  ]
}